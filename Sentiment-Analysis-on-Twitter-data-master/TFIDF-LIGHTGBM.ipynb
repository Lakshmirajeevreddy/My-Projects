{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import enchant\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- arrangeTweet Takes a tweet as Input and normalize each word\n",
    "- for example : 'llooooooovvvee' becomes 'love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = enchant.Dict(\"en_US\")\n",
    "def arrangeTweet(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet=tweet.split()\n",
    "    for i in range(len(tweet)):\n",
    "        if d.check(''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet[i]))):\n",
    "            tweet[i]=''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet[i]))\n",
    "        else:\n",
    "            tweet[i]=''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet[i]))\n",
    "    tweet=' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning of the tweets using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordlist = set(stopwords.words(\"english\"))\n",
    "def clean(tweet):\n",
    "    tweet = re.sub(r\"\\'s\", \" \\'s\", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" \\'ve\", tweet)\n",
    "    tweet = re.sub(r\"n\\'t\", \" n\\'t\", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" \\'re\", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" \\'d\", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" \\'ll\", tweet)\n",
    "    tweet = re.sub(r\",\", \" , \", tweet)\n",
    "    tweet = re.sub(r\"!\", \" ! \", tweet)\n",
    "    tweet = re.sub(r\"\\(\", \" \\( \", tweet)\n",
    "    tweet = re.sub(r\"\\)\", \" \\) \", tweet)\n",
    "    tweet = re.sub(r\"\\?\", \" \\? \", tweet)\n",
    "    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n",
    "    tweet = arrangeTweet(tweet) \n",
    "    liste=[word for word in tweet.split() if word not in stopwordlist]\n",
    "    # Taking the tweets that have length larger than 1\n",
    "    liste=[word for word in liste if len(word)>1]\n",
    "    tweet=' '.join(liste)\n",
    "    return tweet.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we clean the files for the small dataset since we don't have enough memory to build TF-IDF for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "oldfile=open(\"train_pos.txt\",'rb')\n",
    "sentences=[]\n",
    "i=0\n",
    "for line in oldfile:\n",
    "    if i%30000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    \n",
    "    line = line.decode('utf8')\n",
    "    line=clean(line)\n",
    "    #if not (line in sentences):\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(x)['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "oldfile=open(\"train_neg.txt\",'rb')\n",
    "i=0\n",
    "for line in oldfile:\n",
    "    if i%30000==0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    line = line.decode('utf8')\n",
    "    line=clean(line)\n",
    "    #if not (line in sentences):\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffling of the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes=np.arange(0,200000)\n",
    "np.random.shuffle(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=[0]*100000\n",
    "y2=[1]*100000\n",
    "y=y+y2\n",
    "y=np.array(y)\n",
    "y=y[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Scikit-learn we built the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scikit_TFIDF(m,n,Total_clean_train,Total_clean_test):\n",
    "    vectorizer = CountVectorizer(min_df=1,max_features=m,ngram_range=(1,n))\n",
    "    analyze = vectorizer.build_analyzer\n",
    "    X_train = vectorizer.fit_transform(Total_clean_train).toarray()\n",
    "    X_test = vectorizer.transform(Total_clean_test).toarray()\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf_train=transformer.fit_transform(X_train).toarray()\n",
    "    tfidf_test=transformer.transform(X_test).toarray()\n",
    "    return tfidf_train,tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "testfile=open(\"test_data.txt\",'rb')\n",
    "i=0\n",
    "sentences_test=[]\n",
    "for line in testfile:\n",
    "    if i%5000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    line=line.decode('utf8')\n",
    "    line=clean(line)\n",
    "    sentences_test.append((line[line.find(',')+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test=scikit_TFIDF(12000 ,2,sentences,sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=X_train[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x=X_train[170000:]\n",
    "test_y=y[170000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logisticReg = linear_model.LogisticRegression(multi_class='ovr')\n",
    "logisticReg.fit(X_train[0:169999], y[0:169999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predict=logisticReg.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.79      0.81     15668\n",
      "          1       0.78      0.81      0.80     14332\n",
      "\n",
      "avg / total       0.80      0.80      0.80     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_predict,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_clf.fit(X_train[0:169999], y[0:169999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_SVC_linear_predic=lin_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_SVC_linear_predic,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_Bayes = gnb.fit(X_train[0:169999], y[0:169999]).predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_pred_Bayes,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   BUILDING OF LIGHTGBM                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Building the files in the format of LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file=open(\"binary.train\",'w')\n",
    "i=0\n",
    "for i in range(len(y[0:169999])):\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    line=str(y[i])+\"\\t\"+\"\\t\".join([str(x[0]) for x in np.split(X_train[i,:],len(X_train[0,:]))])\n",
    "    #print(line)\n",
    "    if i==169999:\n",
    "        #To prevent the \"\\n\" for the last line\n",
    "        file.write(line)\n",
    "    else:\n",
    "        file.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file=open(\"binary.test\",'w')\n",
    "i=0\n",
    "for i in range(len(y[170000:])):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    line=str(y[i])+\"\\t\"+\"\\t\".join([str(x[0]) for x in np.split(X_train[i,:],len(X_train[0,:]))])\n",
    "    #print(line)\n",
    "    if i==30000:\n",
    "        print(\"hha\")\n",
    "        file.write(line)\n",
    "    else:\n",
    "        print(\"test\")\n",
    "        file.write(line+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file=open(\"pred.test\",'w')\n",
    "i=0\n",
    "for i in range(len(X_test[:,0])):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    line=\"\\t\".join([str(x[0]) for x in np.split(X_test[i,:],len(X_test[0,:]))])\n",
    "    i=i+1\n",
    "    #print(line)\n",
    "    if i==len(X_test[:,0]):\n",
    "        print(\"hha\")\n",
    "        file.write(line)\n",
    "    else:\n",
    "        file.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filen=open(\"pred.txt\",\"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the CSV file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for line in filen:\n",
    "    y_pred.append(int(float(line)>0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=1-2*y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(np.arange(1,10001),y_pred,'predic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
